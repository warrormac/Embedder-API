# ğŸ§  Embedder-API

The **Embedder-API** is a modular FastAPI service for generating semantic embeddings. It supports local models, remote APIs via a self-hosted gateway, or both concurrently, with batch processing and fallback logic. Built to plug into larger document pipelines like Fast-S3Data or any vector database workflow.

---

## ğŸš€ Features

- Accepts single or batch input via `/embed` and `/batch_embed`
- Provider selection: `local`, `gateway`, or `both`
- Dual-path fallback with status feedback when using `both`
- Full support for OpenAI, Gemini via LLM-Gateway
- Unified `EmbedderInterface` with `.embed()` and `.batch_embed()` methods
- Works out-of-the-box with Fast-S3Data CLI
- GPU acceleration supported (if available)

---

## ğŸ“¦ Requirements

- Python 3.8+
- pip

### ğŸ”§ Dependencies (from `requirements.txt`)

```
fastapi
uvicorn
sentence-transformers
pydantic
python-dotenv
```

Install them:

```bash
pip install -r requirements.txt
pip install uvicorn --user
```

---

## ğŸ§  Running the Server

From the project root:

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8001
or
python -m uvicorn main:app --reload --port 8001
```

The server will be available at:  
ğŸ‘‰ http://localhost:8001  
Swagger UI docs:  
ğŸ‘‰ http://localhost:8001/docs

---

## ğŸ“¥ Usage

### 1. `POST /embed`

Request:

```json
{
  "text": "Equimap builds modular AI pipelines.",
  "provider": "gemini"
}
```

### 2. `POST /batch_embed`

Request:

```json
{
  "texts": ["Chunk one.", "Chunk two."],
  "provider": "openai"
}
```

If no provider is specified, it defaults to `"local"`.

---

## ğŸ§© Architecture

```bash
Embedder-API/
â”œâ”€â”€ main.py                      # FastAPI app
â”œâ”€â”€ config.py                    # Central config
â”œâ”€â”€ model_loader.py              # Local model handler
â”œâ”€â”€ .env                         # API keys
â”œâ”€â”€ requirements.txt
â””â”€â”€ providers/
    â”œâ”€â”€ base.py                  # EmbedderInterface
    â”œâ”€â”€ local_provider.py        # Local model (e.g., MiniLM)
    â”œâ”€â”€ gateway_provider.py      # LLM-Gateway handler
    â”œâ”€â”€ openai_provider.py       # OpenAI direct API
    â”œâ”€â”€ gemini_provider.py       # Gemini embeddings
    â”œâ”€â”€ anthropic_provider.py    # [placeholder]
    â”œâ”€â”€ deepseek_provider.py     # [placeholder]
    â””â”€â”€ router.py                # Dynamic routing logic
```

---

## âœ… Capabilities Summary

- Provider options: `local`, `gateway`, `both`
- Full batch support: `batch_embed()`
- Resilient fallback logic in `"both"` mode
- Plug-and-play CLI integration
- FastAPI, async-ready, modular

---

## ğŸ› ï¸ Roadmap

### âœ… Phase 0: Baseline Audit   (DONE)
- Extracted and reviewed code structure
- Verified provider wiring and CLI integration
- Established Embedder-API as a pluggable module

### âœ… Phase 1: Interface Standardization (DONE)
- Created `EmbedderInterface`
- Refactored `local`, `openai`, and `gemini` providers
- Aligned all routes with `.embed()` / `.batch_embed()`
- Added CLI provider selection for Fast-S3Data

### ğŸ”„ Phase 2: Third-Party Gateway Support (ACTIVE)
- Integrate OpenRouter or LLM-Gateway
- Centralize API key management
- Unified routing to OpenAI, Gemini, Anthropic, DeepSeek, etc.

### ğŸ§© Phase 3: Mixed Use (Hybrid Embedding)
- Enable fallback from local â†’ remote (or vice versa)
- Intelligent provider selection (based on size/cost)
- Allow partial remote embedding with priority config

### ğŸ§  Phase 4: Personal Model Integration
- Add loaders for GGUF, HuggingFace, Torch models
- Enable users to register their own local models
- Abstract path-to-model config mapping

---

## ğŸ“„ License

MIT License â€“ use freely and contribute back!

---

## ğŸ™‹â€â™‚ï¸ Maintainer

Created and maintained by **@warrormac**
